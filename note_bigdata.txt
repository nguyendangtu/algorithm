1. hdfs dfs -cat /user/lcrsdev/lcrs/output_data/2018-01-31/01/lcr/RK_20180131_0102/lineitem_stamping/1-CBRC/split/CASH_RESERVE/* | sed 's/null//g' | sed 's/null//g' | hdfs dfs -put -f - /user/lcrsdev/lcrs/output_data/2018-01-31/01/lcr/RK_20180131_0102/lineitem_stamping/1-CBRC/split/CASH_RESERVE.csv
2. #Phoenix Configuration
spark.phoenix.db_url=jdbc:phoenix:x01tbslhdpe1a.vsi.uat.dbs.com:2181:/hbase
spark.phoenix.db_user=
spark.phoenix.db_password=
spark.phoenix.driver=org.apache.phoenix.jdbc.PhoenixDriver
spark.phoenix.zkUrl=x01tbslhdpe1a.vsi.uat.dbs.com
spark.hbase.namenode.url=x01tbslhdpe1a.vsi.uat.dbs.com.vsi.uat.dbs.com
spark.hbase.port=50075
spark.hbase.root.directory=/hbase/data
spark.hbase.zookeeper.quorum=x01tbslhdpe1a.vsi.uat.dbs.com
spark.hbase.zookeeper.znode.parent=/hbase
spark.hbase.zkUrl=jdbc:phoenix:x01tbslhdpe1a.vsi.uat.dbs.com:2181:/hbase
spark.hbase.client.scanner.caching=10000
spark.hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily=512
spark.sql.shuffle.partitions=16
spark.hdfs.url=hdfs://x01tbslhdpe1a.vsi.uat.dbs.com:8020/
spark.data.region.parallel.threads=4
spark.hbase.config.dir=/etc/hbase/conf/
spark.sparse.lookup.max.row.count=200
spark.hbase.namespace=NRD_APP_LCRS
spark.load.process.dir=fileLoadProcess/
spark.folders.toDelete.regionDate=/user/lcrsdev/lcrs/etl
spark.folders.toDelete.regionKey=/user/lcrsdev/lcrs/output_data/cashflow,/user/lcrsdev/lcrs/output_data/cashflow_agg/aggregation/merge,/user/lcrsdev/lcrs/output_data/cashflow_agg/aggregation/no_header,/user/lcrsdev/lcrs/output_data/cashflow_agg/data_lineage/merge,/user/lcrsdev/lcrs/output_data/cashflow_agg/data_lineage/no_header,/user/lcrsdev/lcrs/output_data/cashflow_merge/merge,/user/lcrsdev/lcrs/output_data/cashflow_merge/no_header,/user/lcrsdev/lcrs/output_data/coa_filter/merged,/user/lcrsdev/lcrs/output_data/coa_filter/no_header,/user/lcrsdev/lcrs/output_data/etl/data_file,/user/lcrsdev/lcrs/output_data/etl/file_list
spark.output.filepath=/user/lcrsdev/lcrs/output_data

#spark.devonly.customerid='CSSP_007_S1737279C00'
#spark.devonly.id_number='AGG2_CASP_S1737279C00_L_CA_25101000_85012_SGD_TRANS_T','AGG2_DQSP_S1737279C00_L_SA_25101000_85011_SGD_TRANS_T'
spark.devonly.customerid='TQIO_000_DAO MAU'
#spark.devonly.id_number='CBIO_011_10000942388','CBIO_011_10000944788','CBIO_011_10000947888','CBIO_011_10169783088','CBIO_011_431705080064','CBIO_011_431706050046'
spark.devonly.id_number='TQIO_110_0115283497-1B'
spark.devonly.phase = BIS_MAPPING
spark.devonly.instrument = LOAN
#COA_FILTER,BIS_MAPPING,LI_STAMPING
#LOAN,BOND,CASA,COLLATERAL,TERM_DEPOSIT,OTHER_DEAL,CASH_


3.  if (args.last == "DEV") {
      System.setProperty("java.security.krb5.conf", "resources_dev/krb5.conf")
      UserGroupInformation.loginUserFromKeytab("lcrsdev@REG1.DEV1BANK.DBS.COM", "resources_dev/lcrsdev.keytab")

      val properties = new Properties
      properties.load(new FileInputStream("resources_dev/spark_job.properties"))
      sparkConf.setAll(properties.asScala)
      sparkConf.setMaster("local[2]")
    }
    
4. p:
jdbc:phoenix:x01ghdpeapp1a.sgp.dbs.com,x01ghdpeapp2a.sgp.dbs.com,x01ghdpeapp3a.sgp.dbs.com:2181:/hbase:lcrsapp@REG1.1BANK.DBS.COM:prod\lcrsapp.keytab
org.apache.phoenix.jdbc.PhoenixDriver
d:
jdbc:phoenix:x01tbslhdpe1a.vsi.uat.dbs.com,x01tbslhdpe2a.vsi.uat.dbs.com,x01tbslhdpe3a.vsi.uat.dbs.com:2181:/hbase:lcrsdev@REG1.DEV1BANK.DBS.COM:dev\lcrsdev.keytab
org.apache.phoenix.jdbc.PhoenixDriver


5. pom
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">

	<modelVersion>4.0.0</modelVersion>

	<artifactId>lcrs-engine</artifactId>
	<groupId>com.dbs.lcrs</groupId>
	<version>1.0</version>
	<packaging>jar</packaging>

	<properties>
		<java.version>1.8</java.version>
		<maven.compiler.source>1.8</maven.compiler.source>
		<maven.compiler.target>1.8</maven.compiler.target>
		<scala.binary.version>2.10</scala.binary.version>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
		<junit.version>4.12</junit.version>
		<spark.version>1.6.0</spark.version>
		<scala.tools.version>2.10</scala.tools.version>
		<scala.version>2.10.6</scala.version>
		<scala.compat.version>2.10</scala.compat.version>
		<scoverage.plugin.version>1.3.0</scoverage.plugin.version>
		<sonar.exclusions>**/*/common/*/*,**/*/dataregion/ReplicateData.java</sonar.exclusions>
		<sonar.scoverage.reportPath>target/scoverage.xml</sonar.scoverage.reportPath>
		<scope.provided>compile</scope.provided>
	</properties>
    <repositories>
        <repository>
            <id>dbs</id>
            <!--<name>DBS Artifactory-dev</name>
            <url>http://artifactory.dev.sys.cs.sgp.dbs.com/artifactory/libs-dev/</url> -->
			<name>REPO-D</name>
			<url>https://nexuscimgmt.sgp.dbs.com:8443/nexus/repository/dbsrepo</url>
        </repository>
		<repository>
			<id>lcrs</id>
			<name>REPO-B</name>
			<url>https://nexuscimgmt.sgp.dbs.com:8443/nexus/repository/LCRS</url>
		</repository>
    </repositories>

	<dependencies>


		<dependency>
			<groupId>org.apache.poi</groupId>
			<artifactId>poi</artifactId>
			<version>3.14</version>
		</dependency>
		<dependency>
			<groupId>org.apache.poi</groupId>
			<artifactId>poi-scratchpad</artifactId>
			<version>3.15</version>
		</dependency>
		<dependency>
			<groupId>org.apache.poi</groupId>
			<artifactId>poi-ooxml</artifactId>
			<version>3.14</version>
		</dependency>

		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-library</artifactId>
			<version>${scala.version}</version>
		</dependency>

		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-compiler</artifactId>
			<version>${scala.version}</version>
		</dependency>
		<dependency>
			<groupId>org.scalactic</groupId>
			<artifactId>scalactic_${scala.compat.version}</artifactId>
			<version>3.0.5</version>
		</dependency>
		<dependency>
			<groupId>org.scalatest</groupId>
			<artifactId>scalatest_2.10</artifactId>
			<version>3.0.4</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.scalamock</groupId>
			<artifactId>scalamock_2.10</artifactId>
			<version>4.0.0</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>com.holdenkarau</groupId>
			<artifactId>spark-testing-base_${scala.compat.version}</artifactId>
			<version>${spark.version}_0.8.0</version>
			<scope>test</scope>
			<exclusions>
				<exclusion>
					<groupId>jdk.tools</groupId>
					<artifactId>jdk.tools</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<!--
		<dependency>
			<groupId>org.mockito</groupId>
			<artifactId>mockito-all</artifactId>
			<version>1.10.19</version>
			<scope>test</scope>
		</dependency>
		-->
		<dependency>
			<groupId>org.mockito</groupId>
			<artifactId>mockito-core</artifactId>
			<version>1.9.5</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>com.dbs.lcrs</groupId>
			<artifactId>lcrs-engine-cashflow</artifactId>
			<version>1.0</version>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_${scala.compat.version}</artifactId>
			<version>1.6.0-cdh5.7.1</version>
			<scope>${scope.provided}</scope>
			<exclusions>
				<exclusion>
					<groupId>org.xerial.snappy</groupId>
					<artifactId>snappy-java</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.xerial.snappy</groupId>
			<artifactId>snappy-java</artifactId>
			<version>1.1.7.2</version>
			<type>jar</type>
		</dependency>
		<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql_2.10 -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-sql_${scala.compat.version}</artifactId>
			<version>1.6.0-cdh5.7.1</version>
			<scope>${scope.provided}</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-streaming_2.10</artifactId>
			<version>1.6.0-cdh5.7.1</version>
			<scope>${scope.provided}</scope>
		</dependency>
		<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.10 -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-hive_${scala.compat.version}</artifactId>
			<version>1.6.0-cdh5.7.1</version>
			<scope>${scope.provided}</scope>
		</dependency>
		<!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-spark -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-catalyst_${scala.compat.version}</artifactId>
			<version>1.6.0-cdh5.7.1</version>
			<scope>${scope.provided}</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.commons</groupId>
			<artifactId>commons-lang3</artifactId>
			<version>3.3.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hbase</groupId>
			<artifactId>hbase-spark</artifactId>
			<version>1.2.0-cdh5.7.1</version>
			<scope>${scope.provided}</scope>
		</dependency>
		<!-- https://mvnrepository.com/artifact/com.databricks/spark-csv_2.10 -->
		<dependency>
			<groupId>com.databricks</groupId>
			<artifactId>spark-csv_${scala.compat.version}</artifactId>
			<version>1.4.0</version>
		</dependency>
		<dependency>
			<groupId>org.apache.phoenix</groupId>
			<artifactId>phoenix</artifactId>
			<version>4.7.2-cdh5.11.1-client</version>
			<scope>${scope.provided}</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.maven.plugins</groupId>
			<artifactId>maven-compiler-plugin</artifactId>
			<version>3.7.0</version>
		</dependency>
		<dependency>
			<groupId>com.fasterxml.jackson.core</groupId>
			<artifactId>jackson-annotations</artifactId>
			<version>2.9.6</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hive</groupId>
			<artifactId>hive-common</artifactId>
			<version>3.0.0</version>
		</dependency>
		<!-- <dependency>
			<groupId>org.apache.hive</groupId>
			<artifactId>hive-exec</artifactId>
			<version>3.0.0</version>
		</dependency>
		<dependency>
			<groupId>org.json4s</groupId>
			<artifactId>json4s-jackson_2.12</artifactId>
			<version>3.6.0-M4</version>
		</dependency>-->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-network-shuffle_${scala.compat.version}</artifactId>
            <version>1.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-network-common_${scala.compat.version}</artifactId>
            <version>1.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-launcher_${scala.compat.version}</artifactId>
            <version>1.6.0</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-actor_2.10</artifactId>
            <version>2.3.10</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-remote_2.10</artifactId>
            <version>2.3.10</version>
        </dependency>
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-slf4j_2.10</artifactId>
            <version>2.3.10</version>
        </dependency>
		<dependency>
			<groupId>io.netty</groupId>
			<artifactId>netty-all</artifactId>
			<version>4.1.17.Final</version>
		</dependency>
		<dependency>
			<groupId>com.twitter</groupId>
			<artifactId>parquet-hadoop</artifactId>
			<version>1.6.0</version>
		</dependency>
		<dependency>
			<groupId>com.twitter</groupId>
			<artifactId>parquet-column</artifactId>
			<version>1.6.0</version>
		</dependency>
		<!-- https://mvnrepository.com/artifact/org.datanucleus/datanucleus-core -->
		<!-- <dependency>
			<groupId>org.datanucleus</groupId>
			<artifactId>datanucleus-core</artifactId>
			<version>5.1.9</version>
		</dependency> -->

		<!-- https://mvnrepository.com/artifact/org.datanucleus/datanucleus-api-jdo -->
		<!-- <dependency>
			<groupId>org.datanucleus</groupId>
			<artifactId>datanucleus-api-jdo</artifactId>
			<version>5.1.9</version>
		</dependency> -->
		<!-- https://mvnrepository.com/artifact/org.datanucleus/datanucleus-rdbms -->
		<!-- <dependency>
			<groupId>org.datanucleus</groupId>
			<artifactId>datanucleus-rdbms</artifactId>
			<version>5.1.9</version>
		</dependency> -->
		<!-- https://mvnrepository.com/artifact/com.zaxxer/HikariCP -->
		<dependency>
			<groupId>com.zaxxer</groupId>
			<artifactId>HikariCP</artifactId>
			<version>3.2.0</version>
		</dependency>


		<!--<dependency> <groupId>org.apache.phoenix</groupId> <artifactId>phoenix</artifactId> 
			<version>4.7.0-clabs-phoenix1.3.0-client</version> <scope>${scope.provided}</scope> 
			</dependency> -->
	</dependencies>
	<build>
		<sourceDirectory>src/main/scala</sourceDirectory>
		<finalName>lcrs-engine</finalName>
		<resources>
			<resource>
				<directory>src/main/resources</directory>
			</resource>
		</resources>

		<plugins>
			<plugin>
				<groupId>net.alchim31.maven</groupId>
				<artifactId>scala-maven-plugin</artifactId>
				<version>3.3.1</version>
				<configuration>
					<recompileMode>incremental</recompileMode>
					<useZincServer>true</useZincServer>
					<javacArgs>
						<javacArg>-Xlint:unchecked</javacArg>
						<javacArg>-Xlint:deprecation</javacArg>
						<javacArg>-J-Xms1064m</javacArg>
					</javacArgs>
				</configuration>
				<executions>
					<execution>
						<id>scala-compile-first</id>
						<phase>process-resources</phase>
						<goals>
							<goal>add-source</goal>
							<goal>compile</goal>
						</goals>
					</execution>
					<!--
					<execution>
						<id>scala-test-compile</id>
						<phase>process-test-resources</phase>
						<goals>
							<goal>testCompile</goal>
						</goals>
					</execution>
					-->
				</executions>
			</plugin>
			<plugin>
				<groupId>org.datanucleus</groupId>
				<artifactId>datanucleus-api-jdo</artifactId>
				<version>5.1.9</version>
			</plugin>
			<plugin>
				<groupId>org.datanucleus</groupId>
				<artifactId>datanucleus-rdbms</artifactId>
				<version>5.1.9</version>
			</plugin>
			<!-- https://mvnrepository.com/artifact/com.zaxxer/HikariCP -->
			<plugin>
				<groupId>com.zaxxer</groupId>
				<artifactId>HikariCP</artifactId>
				<version>3.2.0</version>
			</plugin>

			<plugin>
				<groupId>org.apache.phoenix</groupId>
				<artifactId>phoenix</artifactId>
				<version>4.7.2-cdh5.11.1-client</version>
			</plugin>
			<plugin>
				<groupId>com.holdenkarau</groupId>
				<artifactId>spark-testing-base_${scala.compat.version}</artifactId>
				<version>${spark.version}_0.8.0</version>
			</plugin>
			<!-- <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> 
				<version>3.7.0</version> <executions> <execution> <phase>compile</phase> 
				<goals> <goal>compile</goal> </goals> </execution> </executions> </plugin> -->
			<plugin>
				<groupId>org.scoverage</groupId>
				<artifactId>scoverage-maven-plugin</artifactId>
				<version>${scoverage.plugin.version}</version>
				<configuration>
					<scalaVersion>${scala.version}</scalaVersion>
					<highlighting>true</highlighting>
					<excludedPackages>com.dbs.lcrs.common.*;com.dbs.lcrs.common.io.*;com.dbs.lcrs.dataaccess.*;com.dbs.lcrs.datagenration.*;com.dbs.lcrs.datagmigration.util.*;com.dbs.lcrs.udf.*;com.dbs.lcrs.logger.*;com.dbs.lcrs.main.Generate_DDL;com.dbs.lcrs.aggregation.*;com.dbs.lcrs.lr.schema.*;com.dbs.lcrs.lr.bismapping.*;com.dbs.lcrs.lr.lineitemstamping.*;com.dbs.lcrs.lr.reportaggregation.*;com.dbs.lcrs.lr.coafilter.*;com.dbs.lcrs.main.TestMain;com.dbs.lcrs.main.MainWorkFlowRun;com.dbs.lcrs.mapreduce.MapReduceCaller</excludedPackages>
					<excludedFiles>sql</excludedFiles>
					<minimumCoverage>20</minimumCoverage>
					<failOnMinimumCoverage>true</failOnMinimumCoverage>
				</configuration>
				<executions>
					<execution>
						<goals>
							<goal>report</goal> <!-- or integration-check -->
						</goals>
						<phase>prepare-package</phase> <!-- or any other phase -->
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>pl.project13.maven</groupId>
				<artifactId>git-commit-id-plugin</artifactId>
				<version>2.2.2</version>
				<executions>
					<execution>
						<id>get-the-git-infos</id>
						<goals>
							<goal>revision</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
		</plugins>
		<pluginManagement>
			<plugins>
				<!--This plugin's configuration is used to store Eclipse m2e settings 
					only. It has no influence on the Maven build itself. -->
				<plugin>
					<groupId>org.eclipse.m2e</groupId>
					<artifactId>lifecycle-mapping</artifactId>
					<version>1.0.0</version>
					<configuration>
						<lifecycleMappingMetadata>
							<pluginExecutions>
								<pluginExecution>
									<pluginExecutionFilter>
										<groupId>
											net.alchim31.maven
										</groupId>
										<artifactId>
											scala-maven-plugin
										</artifactId>
										<versionRange>
											[3.3.1,)
										</versionRange>
										<goals>
											<goal>add-source</goal>
											<goal>compile</goal>
											<goal>testCompile</goal>
										</goals>
									</pluginExecutionFilter>
									<action>
										<ignore></ignore>
									</action>
								</pluginExecution>
							</pluginExecutions>
						</lifecycleMappingMetadata>
					</configuration>
				</plugin>
			</plugins>
		</pluginManagement>
	</build>


</project>
