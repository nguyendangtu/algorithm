1. hdfs dfs -cat /user/lcrsdev/lcrs/output_data/2018-01-31/01/lcr/RK_20180131_0102/lineitem_stamping/1-CBRC/split/CASH_RESERVE/* | sed 's/null//g' | sed 's/null//g' | hdfs dfs -put -f - /user/lcrsdev/lcrs/output_data/2018-01-31/01/lcr/RK_20180131_0102/lineitem_stamping/1-CBRC/split/CASH_RESERVE.csv
2. #Phoenix Configuration
spark.phoenix.db_url=jdbc:phoenix:x01tbslhdpe1a.vsi.uat.dbs.com:2181:/hbase
spark.phoenix.db_user=
spark.phoenix.db_password=
spark.phoenix.driver=org.apache.phoenix.jdbc.PhoenixDriver
spark.phoenix.zkUrl=x01tbslhdpe1a.vsi.uat.dbs.com
spark.hbase.namenode.url=x01tbslhdpe1a.vsi.uat.dbs.com.vsi.uat.dbs.com
spark.hbase.port=50075
spark.hbase.root.directory=/hbase/data
spark.hbase.zookeeper.quorum=x01tbslhdpe1a.vsi.uat.dbs.com
spark.hbase.zookeeper.znode.parent=/hbase
spark.hbase.zkUrl=jdbc:phoenix:x01tbslhdpe1a.vsi.uat.dbs.com:2181:/hbase
spark.hbase.client.scanner.caching=10000
spark.hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily=512
spark.sql.shuffle.partitions=16
spark.hdfs.url=hdfs://x01tbslhdpe1a.vsi.uat.dbs.com:8020/
spark.data.region.parallel.threads=4
spark.hbase.config.dir=/etc/hbase/conf/
spark.sparse.lookup.max.row.count=200
spark.hbase.namespace=NRD_APP_LCRS
spark.load.process.dir=fileLoadProcess/
spark.folders.toDelete.regionDate=/user/lcrsdev/lcrs/etl
spark.folders.toDelete.regionKey=/user/lcrsdev/lcrs/output_data/cashflow,/user/lcrsdev/lcrs/output_data/cashflow_agg/aggregation/merge,/user/lcrsdev/lcrs/output_data/cashflow_agg/aggregation/no_header,/user/lcrsdev/lcrs/output_data/cashflow_agg/data_lineage/merge,/user/lcrsdev/lcrs/output_data/cashflow_agg/data_lineage/no_header,/user/lcrsdev/lcrs/output_data/cashflow_merge/merge,/user/lcrsdev/lcrs/output_data/cashflow_merge/no_header,/user/lcrsdev/lcrs/output_data/coa_filter/merged,/user/lcrsdev/lcrs/output_data/coa_filter/no_header,/user/lcrsdev/lcrs/output_data/etl/data_file,/user/lcrsdev/lcrs/output_data/etl/file_list
spark.output.filepath=/user/lcrsdev/lcrs/output_data

#spark.devonly.customerid='CSSP_007_S1737279C00'
#spark.devonly.id_number='AGG2_CASP_S1737279C00_L_CA_25101000_85012_SGD_TRANS_T','AGG2_DQSP_S1737279C00_L_SA_25101000_85011_SGD_TRANS_T'
spark.devonly.customerid='TQIO_000_DAO MAU'
#spark.devonly.id_number='CBIO_011_10000942388','CBIO_011_10000944788','CBIO_011_10000947888','CBIO_011_10169783088','CBIO_011_431705080064','CBIO_011_431706050046'
spark.devonly.id_number='TQIO_110_0115283497-1B'
spark.devonly.phase = BIS_MAPPING
spark.devonly.instrument = LOAN
#COA_FILTER,BIS_MAPPING,LI_STAMPING
#LOAN,BOND,CASA,COLLATERAL,TERM_DEPOSIT,OTHER_DEAL,CASH_


3.  if (args.last == "DEV") {
      System.setProperty("java.security.krb5.conf", "resources_dev/krb5.conf")
      UserGroupInformation.loginUserFromKeytab("lcrsdev@REG1.DEV1BANK.DBS.COM", "resources_dev/lcrsdev.keytab")

      val properties = new Properties
      properties.load(new FileInputStream("resources_dev/spark_job.properties"))
      sparkConf.setAll(properties.asScala)
      sparkConf.setMaster("local[2]")
    }
    
4. p:
jdbc:phoenix:x01ghdpeapp1a.sgp.dbs.com,x01ghdpeapp2a.sgp.dbs.com,x01ghdpeapp3a.sgp.dbs.com:2181:/hbase:lcrsapp@REG1.1BANK.DBS.COM:prod\lcrsapp.keytab
org.apache.phoenix.jdbc.PhoenixDriver
d:
jdbc:phoenix:x01tbslhdpe1a.vsi.uat.dbs.com,x01tbslhdpe2a.vsi.uat.dbs.com,x01tbslhdpe3a.vsi.uat.dbs.com:2181:/hbase:lcrsdev@REG1.DEV1BANK.DBS.COM:dev\lcrsdev.keytab
org.apache.phoenix.jdbc.PhoenixDriver
